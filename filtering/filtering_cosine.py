from filtering.filtering_abc import FilterABC
import torch
from sentence_transformers import SentenceTransformer
import torch.nn.functional as F
from typing import List, Tuple


class CosineFilter(FilterABC):
    """
    Filter implementation using embeddings generated by the ModernBERT model.

    This filter computes cosine similarities between the query and candidate documents
    and returns the formatted values if the similarity exceeds a specified threshold.
    """

    def __init__(
        self,
        threshold=0.75,
        batch_size=32,
        truncate_dim=256,
        device="cuda",
        model_name="nomic-ai/modernbert-embed-base",
    ):
        """
        Initialize the ModernBERTEmbedFilter.

        Parameters:
            threshold (float): The cosine similarity threshold for filtering documents.
            batch_size (int): The batch size used during document embedding.
            truncate_dim (int): The maximum dimension length for truncation during embedding.
            device (str): The device to run the model on (e.g., "cuda" or "cpu")
            model_name (strin) : the name of the model to be used, must be supported by SentenceTransformer.
        """
        self.threshold = threshold
        self.batch_size = batch_size
        self.truncate_dim = truncate_dim
        self.device = device

        self.model = SentenceTransformer(
            model_name, device=device, truncate_dim=truncate_dim
        )

        self.queries_per_keyword = {}

    def add_pair(self, keyword: str, value_pair: Tuple[str, str]):
        if not isinstance(keyword, str):
            raise ValueError(f"Keyword must be string, got {type(keyword)}")

        if not isinstance(value_pair, tuple) or len(value_pair) != 2:
            raise ValueError("Value pair must be (text, metadata) tuple")

        if keyword not in self.queries_per_keyword:
            self.queries_per_keyword[keyword] = []

        self.queries_per_keyword[keyword].append(value_pair)

    def _cosine_similarity(
        self, query_emb: torch.Tensor, doc_embs: torch.Tensor
    ) -> torch.Tensor:
        query_emb = F.normalize(query_emb, p=2, dim=-1)
        doc_embs = F.normalize(doc_embs, p=2, dim=-1)
        return torch.mm(query_emb, doc_embs.T).squeeze(0)

    def filter(self) -> List[str]:
        filtered_values = []

        for keyword, pairs in self.queries_per_keyword.items():
            if not pairs:
                continue

            # Prepare texts with proper prefixes
            docs = [f"search_document: {p[0]}" for p in pairs]
            query = [f"search_query: {keyword}"]

            try:
                # Embed query and documents
                with torch.no_grad():
                    query_emb = self.model.encode(query, convert_to_tensor=True)
                    doc_embs = self.model.encode(
                        docs, batch_size=self.batch_size, convert_to_tensor=True
                    )

                # Compute similarities
                similarities = self._cosine_similarity(query_emb, doc_embs)

                # Filter based on threshold
                for idx, score in enumerate(similarities):
                    if score > self.threshold:
                        filtered_values.append(pairs[idx][1])

            except Exception as e:
                print(f"Error processing '{keyword}': {str(e)}")
                continue

        # Clear state after processing
        self.queries_per_keyword.clear()
        return filtered_values
